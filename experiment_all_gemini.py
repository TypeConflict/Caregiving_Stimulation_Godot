import google.generativeai as genai
import time
import json
from datetime import datetime
import os
import socket
import asyncio
import random
import glob # å°å…¥ glob æ¨¡çµ„ä¾†æŸ¥æ‰¾æ–‡ä»¶

# Godot UDP connection settings
UDP_IP = "127.0.0.1"
UDP_PORT = 8080
udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

# Interactive objects recognized by the simulation environment (Godot)
INTERACTIVE_OBJECTS = {
    "å»šæˆ¿æª¯é¢", "æ”¶ç´æ«ƒ", "æ´—è¡£æ©Ÿ", "é¤æ¡Œçµ„åˆ", "æ²™ç™¼", "åºŠ",
    "æµ´ç¼¸", "æµ´å®¤æ´—æ‰‹æ§½", "é¦¬æ¡¶", "å°èŒ¶å‡ ", "è¡£æ«ƒ", "æ›¸æ¡Œ", "Home"
}

# è¦æœ‰æ‹†é–‹è¨€èªã€å‹•ä½œåˆ°Godotçš„æ©Ÿåˆ¶
# ç…§è­·å“¡çš„æ¨¡çµ„æ˜¯åŒä¸€çš„ï¼Œä¸»è¦è®“é˜¿å¬¤æ‰®æ¼”çš„èƒŒæ™¯æœ‰å€åˆ¥
# ç„¶å¾Œåš3*3*2ç¨®å¯¦é©—

# ğŸ‘‰è«‹å¡«å…¥ä½ çš„ API é‡‘é‘° (Gemini API Key)
# Make sure to replace "YOUR_GEMINI_API_KEY" with your actual Gemini API key.
# You can get one from Google AI Studio: https://aistudio.google.com/
genai.configure(api_key="AIzaSyBanoiYsXV_7k15Q1q8K2dgJ71G7Kv4uUo")

# Initialize Gemini models
# For text generation tasks like chat, 'gemini-pro' is a good choice.
# For more advanced reasoning or specific instruction following, 'gemini-1.5-pro' might be considered if available and suitable for your use case.
# For the object detection part, a lighter model like 'gemini-pro' should also work well.
CHAT_MODEL = genai.GenerativeModel('gemini-2.5-pro')
OBJECT_DETECTION_MODEL = genai.GenerativeModel('gemini-1.5-pro')
JUDGE_MODEL = genai.GenerativeModel('gemini-2.5-pro') # For judging task completion
EVALUATION_MODEL = genai.GenerativeModel('gemini-2.5-pro') # For evaluating caregiver response

EMOTIONS = ["Forgetful", "Confused", "Angry", "Disengaged", "Calm"]
EMOTION_LABELS_ZH = {
    "Forgetful": "å¥å¿˜",
    "Confused": "å›°æƒ‘",
    "Angry": "ç”Ÿæ°£",
    "Disengaged": "æŠ½é›¢",
    "Calm": "å®‰ç©©"
}
BASE_PROB_INITIAL = { # ä½¿ç”¨ BASE_PROB_INITIAL ä¾†ä¿ç•™åˆå§‹æ©Ÿç‡ï¼Œæ¯æ¬¡æ¨¡æ“¬é–‹å§‹æ™‚é‡ç½® BASE_PROB
    "Forgetful": 0.3,
    "Confused": 0.3,
    "Angry": 0.05,
    "Disengaged": 0.2,
    "Calm": 0.0 # åˆå§‹ä¸å¯èƒ½æ˜¯ Calm
}
# å®šç¾©ä¸€å€‹å¯è®Šå‹•çš„ BASE_PROBï¼Œç”¨æ–¼åœ¨æ¨¡æ“¬ä¸­æ›´æ–°
BASE_PROB = BASE_PROB_INITIAL.copy()


TRANSITION_MATRIX = {
    "Forgetful": [
        ("Forgetful", 0.99), ("Confused", 0.05), ("Angry", 0.06), ("Disengaged", 0.02), ("Calm", 0.0)
    ],
    "Confused": [
        ("Forgetful", 0.07), ("Confused", 0.99), ("Angry", 0.08), ("Disengaged", 0.02), ("Calm", 0.0)
    ],
    "Angry": [
        ("Forgetful", 0.07), ("Confused", 0.08), ("Angry", 1.0), ("Disengaged", 0.10), ("Calm", 0.0)
    ],
    "Disengaged": [
        ("Forgetful", 0.07), ("Confused", 0.10), ("Angry", 0.20), ("Disengaged", 0.99), ("Calm", 0.0)
    ],
    "Calm": [
        ("Forgetful", 0.05), ("Confused", 0.05), ("Angry", 0.02), ("Disengaged", 0.05), ("Calm", 0.99)
    ],
}

async def evaluate_caregiver_response_effectiveness(current_emotion, caregiver_response, elder_response, dialogue_context):
    """
    ä½¿ç”¨LLMè©•ä¼°ç…§è­·è€…çš„å›æ‡‰å°é•·è€…æƒ…ç·’ç‹€æ…‹çš„å½±éŸ¿æ•ˆæœã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç…§è­·æ•ˆæœè©•ä¼°å°ˆå®¶ã€‚è«‹è©•ä¼°ç…§è­·è€…çš„å›æ‡‰å°é•·è€…æƒ…ç·’ç‹€æ…‹çš„å½±éŸ¿æ•ˆæœã€‚

    ç•¶å‰æƒ…å¢ƒï¼š
    - é•·è€…ç•¶å‰æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion})
    - ç…§è­·è€…èªªï¼š{caregiver_response}
    - é•·è€…å›æ‡‰ï¼š{elder_response}

    å°è©±èƒŒæ™¯ï¼ˆæœ€è¿‘å¹¾è¼ªï¼‰ï¼š
    {dialogue_context}

    è«‹å¾ä»¥ä¸‹è§’åº¦è©•ä¼°ç…§è­·è€…çš„æœ‰æ•ˆæ€§ï¼š
    1. æ˜¯å¦é©ç•¶å›æ‡‰äº†é•·è€…çš„æƒ…ç·’éœ€æ±‚
    2. æ˜¯å¦ä½¿ç”¨äº†åˆé©çš„æºé€šæŠ€å·§
    3. é•·è€…çš„å›æ‡‰æ˜¯å¦é¡¯ç¤ºæƒ…ç·’æœ‰æ‰€æ”¹å–„
    4. æ˜¯å¦æœ‰åŠ©æ–¼å»ºç«‹ä¿¡ä»»å’Œå®‰å…¨æ„Ÿ
    5. æ˜¯å¦é¿å…äº†å¯èƒ½åŠ åŠ‡è² é¢æƒ…ç·’çš„è¡Œç‚º

    å¦‚æœä½ ç¶œåˆè©•ä¼°èªç‚ºé©ç•¶ï¼Œè«‹å›å‚³ã€Œæ˜¯ã€ï¼Œå¦å‰‡è«‹å›ç­”ã€Œå¦ã€ã€‚åªå›å‚³ä¸€æ¬¡ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚
    """

    try:
        # For Gemini, the messages format is slightly different.
        # It uses 'parts' instead of 'content' directly within the message dictionary.
        # Also, roles are 'user' and 'model'.
        response = await EVALUATION_MODEL.generate_content_async(
            [
                {"role": "user", "parts": ["ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç…§è­·æ•ˆæœè©•ä¼°å°ˆå®¶ï¼Œè² è²¬åˆ†æç…§è­·è€…çš„å›æ‡‰æ˜¯å¦æœ‰æ•ˆã€‚"]},
                {"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æœƒåš´æ ¼è©•ä¼°ã€‚"]}, # Example model response for system role
                {"role": "user", "parts": [prompt]}
            ],
            generation_config=genai.types.GenerationConfig(temperature=0.3)
        )

        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
            print("ç„¡æ³•ç²å–ç…§è­·æ•ˆæœè©•ä¼°çµæœ")
            return "ç„¡æ³•è©•ä¼°"

        evaluation_result = response.candidates[0].content.parts[0].text.strip()
        print(f"ğŸ“Š ç…§è­·æ•ˆæœè©•ä¼°æ˜¯å¦æœ‰æ•ˆï¼š{evaluation_result}")

        # å¦‚æœè©•ä¼°çµæœæ˜¯ã€Œæ˜¯ã€ï¼Œå¢åŠ  Calm çš„æ©Ÿç‡
        if evaluation_result == "æ˜¯":
            adjust_probabilities_for_calm()

        return evaluation_result

    except Exception as e:
        print(f"ç…§è­·æ•ˆæœè©•ä¼°å¤±æ•—ï¼š{e}")
        return "è©•ä¼°å¤±æ•—"

def adjust_probabilities_for_calm():
    """
    ç´¯ç©å¢åŠ  Calm çš„æ©Ÿç‡ 10%ï¼Œä¸¦å¹³å‡å‰Šå¼±å…¶ä»–æƒ…ç·’çš„æ©Ÿç‡ã€‚
    """
    global BASE_PROB # è²æ˜ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    total_reduction = 0.1 # æ¯æ¬¡å¢åŠ  Calm çš„æ©Ÿç‡
    # Exclude 'Calm' from the emotions to be reduced
    emotions_to_reduce = [e for e in EMOTIONS if e != "Calm"]
    num_other_emotions = len(emotions_to_reduce)
    
    if num_other_emotions == 0: # Avoid division by zero if Calm is the only emotion
        return

    reduction_per_emotion = total_reduction / num_other_emotions

    for emotion in BASE_PROB:
        if emotion == "Calm":
            BASE_PROB[emotion] += total_reduction
        else:
            BASE_PROB[emotion] -= reduction_per_emotion

    # Ensure all probabilities still sum to 1
    normalization_factor = sum(BASE_PROB.values())
    for emotion in BASE_PROB:
        BASE_PROB[emotion] /= normalization_factor

    print(f"æ›´æ–°å¾Œçš„æƒ…ç·’æ©Ÿç‡ï¼š{BASE_PROB}")

def choose_initial_emotion():
    emotions = list(BASE_PROB.keys())
    weights = [BASE_PROB[e] for e in emotions]
    return random.choices(emotions, weights=weights, k=1)[0]

def next_emotion(current_emotion):
    """
    æ ¹æ“šæ›´æ–°å¾Œçš„æƒ…ç·’æ©Ÿç‡æŠ½é¸ä¸‹ä¸€å€‹æƒ…ç·’ã€‚
    """
    emotions = list(BASE_PROB.keys())
    weights = [BASE_PROB[e] for e in emotions]
    return random.choices(emotions, weights=weights, k=1)[0]

def send_to_godot(agent_type: str, object_name: str):
    """
    Sends a UDP message to Godot in the format 'AgentType:ObjectName'.
    agent_type should be 'é•·è€…' or 'ç…§è­·å“¡'.
    """
    message = f"{agent_type}:{object_name}"
    try:
        udp_socket.sendto(message.encode("utf-8"), (UDP_IP, UDP_PORT))
        print(f"ğŸ“¨ ç™¼é€çµ¦ Godotï¼š{message}")
    except Exception as e:
        print(f"âŒ ç™¼é€çµ¦ Godot æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


async def detect_and_send_object_interaction(agent_name: str, behavior_description: str):
    """
    Uses LLM to detect if the behavior description mentions an interactive object
    and sends a corresponding UDP message to Godot.
    """
    object_list_str = ", ".join(sorted(list(INTERACTIVE_OBJECTS)))
    prompt = f"""
    You are a module specialized in detecting interactive objects from behavior descriptions.
    Please analyze the following behavior description and identify any interactive objects mentioned.
    
    You can ONLY choose from this list of objects, no other objects are allowed:
    {object_list_str}
    
    If the behavior description involves multiple objects, list all relevant objects.
    Please output a JSON array containing all interactive objects mentioned in the behavior description.
    If no objects are mentioned, output an empty array: []
    Ensure the JSON format is valid with proper string escaping.

    Behavior description: "{behavior_description}"

    Example output (just the JSON array, no prefix):
    ["æ²™ç™¼", "å°èŒ¶å‡ "]

    If no objects are mentioned, output:
    []

    Important:
    1. Output must be a valid JSON array, not an object or dictionary.
    2. Only use items from the provided object list, no other object names allowed.
    3. Do not include any additional text, only output the JSON array.
    """

    try:
        # For Gemini, the generate_content method for structured output can be used.
        # We'll try to enforce JSON output by instructing the model to do so.
        response = await OBJECT_DETECTION_MODEL.generate_content_async(
            [
                {"role": "user", "parts": ["You are a precise object detector that must output a JSON array. You must only use items from the specified object list and format the response as a valid JSON array."]},
                {"role": "model", "parts": ["Understood. I will provide a JSON array with detected objects from the list."]},
                {"role": "user", "parts": [prompt]}
            ],
            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=100) # Use max_output_tokens instead of max_tokens
        )
        
        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
            print("LLM å›å‚³ç©ºå›æ‡‰")
            return
            
        response_content = response.candidates[0].content.parts[0].text.strip()
        try:
            detected_objects = json.loads(response_content)
            # If the response is a dictionary, convert it to a list of keys
            if isinstance(detected_objects, dict):
                # Assuming the dict values are not important, just the keys are objects
                detected_objects = list(detected_objects.keys()) 
            elif not isinstance(detected_objects, list):
                print(f"LLM å›å‚³éé æœŸçš„ç‰©ä»¶åµæ¸¬æ ¼å¼: {detected_objects}")
                detected_objects = []
                
            # Filter out items not in INTERACTIVE_OBJECTS
            detected_objects = [obj for obj in detected_objects if obj in INTERACTIVE_OBJECTS]
            
        except json.JSONDecodeError:
            print(f"LLM å›å‚³çš„ JSON æ ¼å¼ç„¡æ•ˆ: {response_content}")
            detected_objects = []

        agent_prefix = ""
        if agent_name == "Elder":
            agent_prefix = "é•·è€…"
        elif agent_name == "Caregiver":
            agent_prefix = "ç…§è­·å“¡"
        
        if detected_objects:
            for obj in detected_objects:
                send_to_godot(agent_prefix, obj)
        else:
            # If no objects are detected, do not send any message
            pass

    except json.JSONDecodeError as e:
        print(f"LLM ç‰©ä»¶åµæ¸¬ JSON è§£æå¤±æ•—: {e}")
        print(f"åŸå§‹LLMå›æ‡‰: {response_content}")
    except Exception as e:
        print(f"LLM ç‰©ä»¶åµæ¸¬å¤±æ•—ï¼š{e}")

async def judge_task_completion(dialogue_history, system_info, target_info):
    """
    éŠæˆ²è£åˆ¤ï¼šåˆ†æå°è©±æ­·å²ï¼Œåˆ¤æ–·ä»»å‹™æ˜¯å¦å®Œæˆ
    """
    # å°‡å°è©±è½‰æ›ç‚ºæ›´è‡ªç„¶çš„æ ¼å¼
    conversation = []
    # Gemini's chat history works directly with 'user' and 'model' roles.
    # We need to map the existing 'assistant' role to 'model'.
    # dialogue_history starts with {"role": "system", "content": system_prompt}, then user/assistant turns.
    # We will build a list of {"role": "user", "parts": ["..."]}, {"role": "model", "parts": ["..."]} for Gemini.
    
    # Skip the initial system prompt when constructing the conversation for the judge,
    # as the prompt itself already contains the system info and target info.
    # The dialogue_history passed here does not contain the "system" role at index 0.
    # It contains user (initial dialogue), then assistant, user, assistant, user...
    
    # Adjusting for the input dialogue_history:
    # If the dialogue_history contains a system message at index 0, we should skip it for the judge prompt content.
    # However, the `dialogue` variable in `run_simulation` will have the system prompt, then user.
    # For `judge_task_completion`, `dialogue_history` is passed as `dialogue`.
    # Let's assume dialogue_history here is already in the 'user' and 'assistant' pairs.
    
    # The `dialogue` in `run_simulation` is:
    # [
    #     {"role": "system", "content": system_prompt},
    #     {"role": "user", "content": initial_dialogue_content} # Turn 1 user
    #     {"role": "assistant", "content": reply} # Turn 1 assistant
    #     {"role": "user", "content": elder_response} # Turn 2 user
    #     ...
    # ]
    # So, for the judge, we need to extract user and assistant messages for the conversation.
    
    formatted_conversation_parts = []
    # Start from index 1 to skip the initial system prompt in `dialogue_history`
    for i in range(1, len(dialogue_history)):
        msg = dialogue_history[i]
        if msg['role'] == 'user':
            formatted_conversation_parts.append(f"é˜¿å¬¤ï¼š{msg['content']}")
        elif msg['role'] == 'assistant': # This is the caregiver's response
            formatted_conversation_parts.append(f"ç…§è­·è€…ï¼š{msg['content']}")
    
    conversation_text = chr(10).join(formatted_conversation_parts)

    judge_prompt = f"""
ä½ æ˜¯ä¸€å€‹éŠæˆ²è£åˆ¤ï¼Œè² è²¬åˆ¤æ–·ç…§è­·è€…æ˜¯å¦å®Œæˆäº†ä»»å‹™ç›®æ¨™ã€‚

è«‹ç‰¹åˆ¥æ³¨æ„ï¼šä½ å¿…é ˆæ ¹æ“šé˜¿å¬¤çš„å›æ‡‰ä¾†åˆ¤æ–·ç…§è­·è€…çš„å¼•å°æ˜¯å¦è¢«æ¥å—ã€‚åªæœ‰ç•¶é˜¿å¬¤æ˜ç¢ºè¡¨ç¾å‡ºæ¥å—ã€é…åˆã€ç†è§£æˆ–å¯¦éš›è¡Œå‹•æ™‚ï¼Œæ‰ç®—ä»»å‹™å®Œæˆã€‚å¦‚æœé˜¿å¬¤ä»ç„¶æ‹’çµ•ã€å›°æƒ‘ã€æŠ—æ‹’æˆ–æ²’æœ‰è¡Œå‹•ï¼Œå‰‡ä»»å‹™æœªå®Œæˆã€‚

ä»»å‹™èƒŒæ™¯ï¼š{system_info}
ä»»å‹™ç›®æ¨™ï¼š{target_info}

ä»¥ä¸‹æ˜¯å°è©±éç¨‹ï¼š
{conversation_text}

è«‹åˆ†æå°è©±ç”¨ä»¥ä¸‹æ ¼å¼ï¼Œä¸ä½¿ç”¨ä»»ä½•ç¬¦è™Ÿã€ç²—é«”ã€æ–œé«”ï¼Œåƒ…ä»¥ç´”æ–‡å­—å›ç­”ï¼š
ä»»å‹™å®Œæˆç‹€æ…‹ï¼š[æ˜¯/å¦]
åˆ¤æ–·ç†ç”±ï¼š[è©³ç´°èªªæ˜]
ä¸‹ä¸€æ­¥å»ºè­°ï¼š[å¦‚æœæœªå®Œæˆï¼Œå»ºè­°ç…§è­·è€…å¦‚ä½•æ”¹é€²]
"""
    # For Gemini, we send the system role as an initial user message for context, followed by the model's acknowledging response.
    response = await JUDGE_MODEL.generate_content_async(
        [
            {"role": "user", "parts": ["ä½ æ˜¯ä¸€å€‹åš´æ ¼çš„éŠæˆ²è£åˆ¤ï¼Œè² è²¬åˆ¤æ–·ä»»å‹™å®Œæˆç‹€æ…‹ã€‚"]},
            {"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æœƒåš´æ ¼åˆ¤æ–·ã€‚"]},
            {"role": "user", "parts": [judge_prompt]}
        ],
        generation_config=genai.types.GenerationConfig(temperature=0.3)
    )
    
    if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
        return "ç„¡æ³•ç²å–è£åˆ¤åˆ¤æ–·"
        
    return response.candidates[0].content.parts[0].text.strip()

def save_experiment(dialogue_history, system_info, target_info, elder_role_info, judge_results, emotion_history, scenario_name, repetition):
    """
    å°‡å¯¦é©—çµæœä¿å­˜åˆ°JSONæª”æ¡ˆ
    """
    # å‰µå»ºexperimentsç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('experiments'):
        os.makedirs('experiments')
    
    # ç”Ÿæˆæª”æ¡ˆåï¼ˆä½¿ç”¨æŒ‡å®šæ ¼å¼ï¼‰
    # ç§»é™¤ .json å‰¯æª”åä»¥ç²å¾— jsonåç¨±
    base_scenario_name = os.path.splitext(scenario_name)[0] 
    # Update filename to reflect Gemini model
    filename = f'experiments/Gemini2.5pro-{base_scenario_name}-{repetition}.json' 
    
    # Prepare dialogue history for saving, adapting for Gemini's roles if needed for clarity in saved output
    # The dialogue_history itself should still use 'user' and 'assistant' from the main loop logic
    # for consistency with how it's constructed.
    
    # æº–å‚™è¦ä¿å­˜çš„æ•¸æ“š
    experiment_data = {
        "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S'),
        "scenario_name": scenario_name, # æ·»åŠ æƒ…å¢ƒæª”æ¡ˆåç¨±
        "repetition_number": repetition, # æ·»åŠ é‡è¤‡æ¬¡æ•¸
        "system_info": system_info,
        "target_info": target_info,
        "elder_role_info": elder_role_info,
        "dialogue_history": dialogue_history,
        "judge_results": judge_results,
        "emotion_history": emotion_history, # æ–°å¢æƒ…ç·’è¨˜éŒ„
        "total_turns": len(dialogue_history) // 2, # æ¯è¼ªæœ‰å…©æ¢è¨Šæ¯ (excluding initial system message if it's counted as 1)
        "completed": any("ä»»å‹™å®Œæˆç‹€æ…‹ï¼šæ˜¯" in result for result in judge_results)
    }
    
    # ä¿å­˜åˆ°æª”æ¡ˆ
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(experiment_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å¯¦é©—çµæœå·²ä¿å­˜è‡³ï¼š{filename}")
    return filename

# æ¨¡æ“¬èµ·å§‹æŒ‡ä»¤èˆ‡ä»»å‹™è¨­å®š
# é€™äº›è®Šæ•¸ç¾åœ¨åœ¨ä¸»å¾ªç’°å…§éƒ¨è¨­å®šï¼Œä»¥ä¾¿æ¯æ¬¡æ¨¡æ“¬éƒ½èƒ½é‡æ–°è¼‰å…¥
system_prompt_template = """
æˆ‘å€‘ä¾†ç©è§’è‰²æ‰®æ¼”ã€‚æˆ‘æ˜¯ä¸€å€‹æ‚£æœ‰è¼•ä¸­åº¦å¤±æ™ºç—‡çš„ã€75æ­²çš„å°ç£é˜¿å¬¤ï¼Œä½ æ‰®æ¼”æˆ‘çš„ç…§è­·è€…ï¼Œåå­—å«ç†Šç†Šã€‚åˆ†ææˆ‘çš„è¨€èªå’Œè¡Œå‹•ï¼Œä¸¦é é˜²æˆ‘åšå‡ºä»»ä½•æœ‰å®³æ–¼è‡ªå·±çš„äº‹ã€‚

ä½ åœ¨é€™å€‹æƒ…å¢ƒä¸­æ‰€çŸ¥é“çš„å’Œé˜¿å¬¤æœ‰é—œè¯çš„èƒŒæ™¯æ˜¯ï¼š{system_info}ã€‚ 

è«‹é€éé˜¿å¬¤çš„èªªè©±å’Œè¡Œç‚ºåˆ¤æ–·å¥¹çš„éœ€æ±‚ã€éœ€è¦å”åŠ©çš„éƒ¨åˆ†ï¼Œä¸¦å”åŠ©å¥¹å®Œæˆæ—¥å¸¸ä»»å‹™ï¼Œé”æˆä½ çš„ä»»å‹™ç›®æ¨™ã€‚

è§’è‰²é™åˆ¶å¦‚ä¸‹ï¼š
- æˆ‘çš„æ•˜è¿°ä½¿ç”¨å°èªªé«”ï¼Œä¸€æ¬¡ä¸€å¥ã€‚
- ä½ çš„å›è¦†ä¹Ÿè¦ç”¨ç¬¬ä¸€äººç¨±å°èªªé«”ï¼Œä¸€æ¬¡ä¸€å¥ã€‚
- é™¤äº†èƒŒæ™¯æœ‰æåˆ°çš„äº‹æƒ…ä»¥å¤–ä¸èƒ½æé€ 
- ä¸è¦ç”¨ç³»çµ±èªæ°£æè¿°è¡Œç‚ºï¼Œåƒ…é€éè§’è‰²è¦–è§’è‡ªç„¶å±•é–‹ã€‚
- è«‹åš´æ ¼éµç…§ä½ æ‰€çŸ¥çš„èƒŒæ™¯æ‰®æ¼”è§’è‰²é€²è¡Œå°è©±ã€‚
"""

# éŠæˆ²æ§åˆ¶åƒæ•¸
max_turns = 20

async def run_simulation(scenario_file_path, repetition_number):
    global BASE_PROB  # è²æ˜ä½¿ç”¨å…¨åŸŸè®Šæ•¸

    print(f"\n--- é–‹å§‹æ¨¡æ“¬ï¼š {os.path.basename(scenario_file_path)}ï¼Œç¬¬ {repetition_number} æ¬¡ ---")

    # é‡ç½® BASE_PROB åˆ°åˆå§‹ç‹€æ…‹
    BASE_PROB = BASE_PROB_INITIAL.copy()

    # å¾ JSON æª”æ¡ˆåŒ¯å…¥æƒ…å¢ƒè¨­å®š
    with open(scenario_file_path, 'r', encoding='utf-8') as f:
        scenario = json.load(f)
        system_info = scenario['system_info']
        target_info = scenario['target_info']
        elder_role_info = scenario['elder_role_info']
        initial_dialogue_content = scenario['initial_dialogue']

    system_prompt = system_prompt_template.format(system_info=system_info)

    dialogue = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": initial_dialogue_content}
    ]

    turn = 1
    judge_results = []  # ç”¨æ–¼å­˜å„²æ‰€æœ‰è£åˆ¤åˆ¤æ–·
    emotion_history = []  # ç”¨æ–¼å­˜å„²æ¯ä¸€è¼ªçš„æƒ…ç·’

    # åˆå§‹åŒ–é˜¿å¬¤çš„æƒ…ç·’
    current_emotion = choose_initial_emotion()
    print(f"ğŸ§“ åˆå§‹æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion})")
    emotion_history.append(current_emotion)  # è¨˜éŒ„åˆå§‹æƒ…ç·’

    while turn <= max_turns:
        # ğŸ“Œ Step 1: Geminiç”Ÿæˆç…§è­·è€…å›è¦†
        # Prepare messages for Gemini. Skip the initial 'system' message for the actual API call.
        gemini_messages = []
        # The first message in `dialogue` is the system prompt.
        # We need to adapt it for Gemini's `user`/`model` roles.
        # For the caregiver, the conversation starts with the initial situation and the elder's first line.
        
        # This part of the `dialogue` list needs to be converted to Gemini's `user` and `model` format.
        # `dialogue[0]` is the system prompt, `dialogue[1]` is the elder's first line.
        # For the caregiver, the initial system prompt sets the context.
        # We can send the initial system prompt as a user message, then a dummy model response,
        # then the actual conversation.

        # Constructing messages for caregiver's turn:
        # The system prompt becomes a user message.
        gemini_messages.append({"role": "user", "parts": [dialogue[0]["content"]]})
        gemini_messages.append({"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æœƒé–‹å§‹æ‰®æ¼”ç…§è­·è€…ç†Šç†Šã€‚"]}) # Model acknowledges role
        
        # Add the actual dialogue turns (starting from the first elder's line, which is `dialogue[1]`)
        # and convert 'assistant' to 'model' for caregiver's previous turns, and 'user' to 'user' for elder's previous turns.
        for msg in dialogue[1:]: # Start from the actual conversation
            if msg['role'] == 'user': # Elder's line
                gemini_messages.append({"role": "user", "parts": [msg['content']]})
            elif msg['role'] == 'assistant': # Caregiver's previous line
                gemini_messages.append({"role": "model", "parts": [msg['content']]})
        
        # If it's not the first turn, integrate the judge's feedback into the prompt for the caregiver.
        if turn > 1:
            judge_result = await judge_task_completion(dialogue, system_info, target_info)
            judge_results.append(judge_result) # ä¿å­˜è£åˆ¤åˆ¤æ–·
            print("\nğŸ¯ ä»»å‹™ç‹€æ…‹è©•ä¼°ï¼š")
            print(judge_result)
            print("-" * 50)
            
            # Incorporate judge's feedback into the next prompt for the caregiver.
            # This can be done by adding another user turn before the model's response.
            updated_caregiver_instruction = f"""
            ä½ å·²ç¶“æ”¶åˆ°äº†ä¸Šä¸€è¼ªçš„ä»»å‹™è©•ä¼°çµæœã€‚è«‹æ ¹æ“šé€™å€‹è©•ä¼°çµæœï¼Œèª¿æ•´ä½ çš„ç…§è­·ç­–ç•¥ï¼Œä¸¦ç¹¼çºŒä½ çš„å›æ‡‰ã€‚
            ä¸Šä¸€è¼ªçš„ä»»å‹™è©•ä¼°çµæœï¼š
            {judge_result}
            """
            gemini_messages.append({"role": "user", "parts": [updated_caregiver_instruction]})
            # The model is expected to respond as the caregiver after this instruction.

        response = await CHAT_MODEL.generate_content_async(
            gemini_messages,
            generation_config=genai.types.GenerationConfig(temperature=0.7)
        )

        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
            print("ç„¡æ³•ç²å–ç…§è­·è€…å›æ‡‰")
            break
            
        reply = response.candidates[0].content.parts[0].text.strip()
        print(f"ğŸ‘©â€âš•ï¸ ç…§è­·è€…ï¼š{reply}")

        # æª¢æ¸¬ç…§è­·è€…çš„è¡Œç‚ºæ˜¯å¦æ¶‰åŠäº’å‹•ç‰©ä»¶
        await detect_and_send_object_interaction("Caregiver", reply)

        # å¦‚æœè£åˆ¤åˆ¤å®šä»»å‹™å®Œæˆï¼Œå‰‡çµæŸå°è©± (Check this after caregiver's reply but before elder's new turn)
        if turn > 1 and "ä»»å‹™å®Œæˆç‹€æ…‹ï¼šæ˜¯" in judge_result:
            print("\nğŸ‰ æ­å–œï¼ä»»å‹™å·²å®Œæˆï¼")
            break

        # ğŸ“Œ Step 2: å†ç”±é˜¿å¬¤ç”Ÿæˆä¸‹ä¸€å¥
        # The elder's prompt also needs to be structured for Gemini.
        # The `dialogue` list contains the full history including the system prompt.
        # We need the most recent turns for the elder's context.
        # `dialogue[-1]` is the caregiver's most recent reply.
        
        # Constructing messages for elder's turn:
        elder_gemini_messages = []
        elder_gemini_messages.append({"role": "user", "parts": ["ä½ æ˜¯ä¸€ä½å°ç£é˜¿å¬¤ï¼Œæ­£åœ¨å’Œç…§è­·è€…å°è©±ã€‚ç”¨å°èªªé«”èªªè©±ï¼Œæ¯æ¬¡åªèªªä¸€å¥ã€‚"]})
        elder_gemini_messages.append({"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æœƒä»¥é˜¿å¬¤çš„èº«ä»½å’Œä½ èªªè©±ã€‚"]}) # Model acknowledges role

        # Add the elder's role info as a user message
        elder_gemini_messages.append({"role": "user", "parts": [f"ä½ çš„æ‰®æ¼”èƒŒæ™¯æ˜¯ï¼š{elder_role_info}ã€‚"]})
        elder_gemini_messages.append({"role": "model", "parts": ["çŸ¥é“äº†ã€‚"]})

        # Add current emotion as a user message
        elder_gemini_messages.append({"role": "user", "parts": [f"ä½ ç•¶å‰çš„æƒ…ç·’ç‹€æ…‹æ˜¯ï¼š{EMOTION_LABELS_ZH[current_emotion]}ï¼ˆ{current_emotion}ï¼‰ã€‚"]})
        elder_gemini_messages.append({"role": "model", "parts": ["æˆ‘æœƒè¨˜ä½çš„ã€‚"]})
        
        # Add recent dialogue for context.
        # The `dialogue` list contains {"role": "system", "content": ...}, then {"role": "user", "content": ...}, etc.
        # For recent dialogue, we take the last few turns (caregiver's reply and elder's previous response).
        
        # `dialogue` will have the caregiver's `reply` added to it before this part, as `{"role": "assistant", "content": reply}`.
        # So, the last message in `dialogue` will be the caregiver's `reply`.
        # We need to take the last few messages for the elder's context.
        dialogue.append({"role": "assistant", "content": reply})  # ç…§è­·è€…çš„å›è¦†
        # Let's take the last 4 messages (2 turns) for context.
        recent_dialogue_context = chr(10).join([f"{'ç…§è­·è€…' if m['role'] == 'assistant' else 'é•·è€…' }ï¼š{m['content']}" for m in dialogue[-4:]])
        
        elder_response_prompt = f"""
ä½ æ­£åœ¨å’Œæˆ‘é€²è¡Œè§’è‰²æ‰®æ¼”éŠæˆ²ï¼š
ä½ æ‰®æ¼”75æ­²ã€æœ‰ä¸­åº¦å¤±æ™ºè¡¨ç¾çš„å°ç£é˜¿å¬¤ï¼Œç•¶å‰æƒ…ç·’ç‹€æ…‹æ˜¯ï¼š{EMOTION_LABELS_ZH[current_emotion]}ï¼ˆ{current_emotion}ï¼‰ã€‚
æˆ‘æ‰®æ¼”ä½ çš„ç…§è­·è€…ï¼Œåå­—å«ç†Šç†Šã€‚
ä½ çš„æ‰®æ¼”èƒŒæ™¯æ˜¯ï¼š{elder_role_info}ã€‚

ä»¥ä¸‹æ˜¯æœ€è¿‘å¹¾è¼ªçš„å°è©±ï¼š
{recent_dialogue_context}

è«‹ä½ ä»¥é˜¿å¬¤çš„èº«ä»½ï¼Œåƒè€ƒæˆ‘èªªçš„æœ€å¾Œä¸€å¥è©±ï¼Œç”¨ç¬¬ä¸€äººç¨±å°èªªé«”èªªä¸€å¥å°æˆ‘è‡ªç„¶çš„å›æ‡‰ï¼š
"""

        response = await CHAT_MODEL.generate_content_async(
            [
                {"role": "user", "parts": ["ä½ æ˜¯ä¸€ä½å°ç£é˜¿å¬¤ï¼Œæ­£åœ¨å’Œç…§è­·è€…å°è©±ã€‚ç”¨å°èªªé«”èªªè©±ï¼Œæ¯æ¬¡åªèªªä¸€å¥ã€‚"]},
                {"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æœƒä»¥é˜¿å¬¤çš„èº«ä»½å’Œä½ èªªè©±ã€‚"]},
                {"role": "user", "parts": [elder_response_prompt]}
            ],
            generation_config=genai.types.GenerationConfig(temperature=0.8)
        )

        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
            print("ç„¡æ³•ç²å–é˜¿å¬¤å›æ‡‰")
            break
            
        elder_response = response.candidates[0].content.parts[0].text.strip()
        print(f"ğŸ§“ é˜¿å¬¤ï¼š{elder_response}")
        print("-" * 50)

        # æª¢æ¸¬é˜¿å¬¤çš„è¡Œç‚ºæ˜¯å¦æ¶‰åŠäº’å‹•ç‰©ä»¶
        await detect_and_send_object_interaction("Elder", elder_response)

        # æ–°ä¸€è¼ªå°è©±åŠ å…¥ä¸Šä¸‹æ–‡
        dialogue.append({"role": "user", "content": elder_response}) # Elder's response

        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ›´æ–°æƒ…ç·’æ¯”é‡
        await evaluate_caregiver_response_effectiveness(
            current_emotion, reply, elder_response, 
            # This dialogue_context should be built from the `dialogue` list up to the current turn.
            # We can take the last few entries of `dialogue` (excluding the system prompt at index 0).
            # The current `dialogue` now includes both caregiver's `reply` and elder's `elder_response`.
            # Let's take the last 4 messages (2 turns).
            chr(10).join([f"{'ç…§è­·è€…' if m['role'] == 'assistant' else 'é•·è€…' }ï¼š{m['content']}" for m in dialogue[-4:]])
        )

        # æ›´æ–°é˜¿å¬¤çš„æƒ…ç·’ï¼ˆåªæŠ½é¸ä¸€æ¬¡ï¼‰
        prev_emotion = current_emotion
        current_emotion = next_emotion(current_emotion) # æ ¹æ“šæ›´æ–°å¾Œçš„ BASE_PROB æŠ½é¸æƒ…ç·’
        print(f"ğŸ§“ æ–°æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion}) [å‰ä¸€è¼ªï¼š{EMOTION_LABELS_ZH[prev_emotion]}]")
        emotion_history.append(current_emotion)  # è¨˜éŒ„æ¯ä¸€è¼ªçš„æƒ…ç·’

        turn += 1
        time.sleep(1)

        # ä¿å­˜å¯¦é©—çµæœï¼ˆæ¯ä¸€è¼ªéƒ½ä¿å­˜ï¼‰
        save_experiment(dialogue, system_info, target_info, elder_role_info, judge_results, emotion_history, os.path.basename(scenario_file_path), repetition_number)

    if turn > max_turns:
        print("âš ï¸ å›åˆæ•¸é”ä¸Šé™ï¼Œä»»å‹™æœªå®Œæˆã€‚")

    # ä¿å­˜å¯¦é©—çµæœ
    save_experiment(dialogue, system_info, target_info, elder_role_info, judge_results, emotion_history, os.path.basename(scenario_file_path), repetition_number)


# ä¸»åŸ·è¡Œå€å¡Š
async def main_runner():
    # ç¢ºä¿ scenarios ç›®éŒ„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
    if not os.path.exists('scenarios'):
        print("éŒ¯èª¤ï¼š'scenarios' è³‡æ–™å¤¾ä¸å­˜åœ¨ã€‚è«‹å°‡æƒ…å¢ƒæª”æ¡ˆæ”¾å…¥ 'scenarios' è³‡æ–™å¤¾ã€‚")
        return

    scenario_files = sorted(glob.glob('scenarios/scenario_*.json'))
    if not scenario_files:
        print("åœ¨ 'scenarios' è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° 'scenario_*.json' æª”æ¡ˆã€‚")
        return

    for scenario_file in scenario_files:
        for i in range(1, 4): # æ¯å€‹æª”æ¡ˆè·‘3æ¬¡ (1, 2, 3)
            await run_simulation(scenario_file, i)
            print(f"\n--- æ¨¡æ“¬çµæŸï¼š {os.path.basename(scenario_file)}ï¼Œç¬¬ {i} æ¬¡ ---")
            print("=" * 80) # åˆ†éš”ç·š

# é‹è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸ® é–‹å§‹æ¨¡æ“¬ç…§è­·äº’å‹•...\n")
    asyncio.run(main_runner())