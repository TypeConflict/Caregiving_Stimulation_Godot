import openai
import time
import json
from datetime import datetime
import os
import socket
import asyncio
import random
import glob # å°å…¥ glob æ¨¡çµ„ä¾†æŸ¥æ‰¾æ–‡ä»¶

# å°å…¥ Google Generative AI å‡½å¼åº«
import google.generativeai as genai

# Godot UDP connection settings
UDP_IP = "127.0.0.1"
UDP_PORT = 8080
udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

# Interactive objects recognized by the simulation environment (Godot)
INTERACTIVE_OBJECTS = {
   "å»šæˆ¿æª¯é¢", "æ”¶ç´æ«ƒ", "æ´—è¡£æ©Ÿ", "é¤æ¡Œçµ„åˆ", "æ²™ç™¼", "åºŠ",
   "æµ´ç¼¸", "æµ´å®¤æ´—æ‰‹æ§½", "é¦¬æ¡¶", "å°èŒ¶å‡ ", "è¡£æ«ƒ", "æ›¸æ¡Œ", "Home"
}

# è¦æœ‰æ‹†é–‹è¨€èªã€å‹•ä½œåˆ°Godotçš„æ©Ÿåˆ¶
# ç…§è­·å“¡çš„æ¨¡çµ„æ˜¯åŒä¸€çš„ï¼Œä¸»è¦è®“é˜¿å¬¤æ‰®æ¼”çš„èƒŒæ™¯æœ‰å€åˆ¥
# ç„¶å¾Œåš3*3*2ç¨®å¯¦é©—

# ğŸ‘‰è«‹å¡«å…¥ä½ çš„ API é‡‘é‘°
# OpenAI API é‡‘é‘° (ç”¨æ–¼ GPT æ¨¡å‹)
openai_client = openai.OpenAI(api_key="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")

# Google Gemini API é‡‘é‘° (ç”¨æ–¼ Gemini æ¨¡å‹)
# è«‹å‹™å¿…æ›¿æ›ç‚ºæ‚¨è‡ªå·±çš„ Gemini API é‡‘é‘°
genai.configure(api_key="AIzaSyBanoiYsXV_7k15Q1q8K2dgJ71G7Kv4uUo") # <<< åœ¨é€™è£¡å¡«å…¥æ‚¨çš„ Gemini API é‡‘é‘°

EMOTIONS = ["Forgetful", "Confused", "Angry", "Disengaged", "Calm"]
EMOTION_LABELS_ZH = {
    "Forgetful": "å¥å¿˜",
    "Confused": "å›°æƒ‘",
    "Angry": "ç”Ÿæ°£",
    "Disengaged": "æŠ½é›¢",
    "Calm": "å®‰ç©©"
}
BASE_PROB_INITIAL = { # ä½¿ç”¨ BASE_PROB_INITIAL ä¾†ä¿ç•™åˆå§‹æ©Ÿç‡ï¼Œæ¯æ¬¡æ¨¡æ“¬é–‹å§‹æ™‚é‡ç½® BASE_PROB
    "Forgetful": 0.3,
    "Confused": 0.3,
    "Angry": 0.05,
    "Disengaged": 0.2,
    "Calm": 0.0 # åˆå§‹ä¸å¯èƒ½æ˜¯ Calm
}
# å®šç¾©ä¸€å€‹å¯è®Šå‹•çš„ BASE_PROBï¼Œç”¨æ–¼åœ¨æ¨¡æ“¬ä¸­æ›´æ–°
BASE_PROB = BASE_PROB_INITIAL.copy()


TRANSITION_MATRIX = {
    "Forgetful": [
        ("Forgetful", 0.99), ("Confused", 0.05), ("Angry", 0.06), ("Disengaged", 0.02), ("Calm", 0.0)
    ],
    "Confused": [
        ("Forgetful", 0.07), ("Confused", 0.99), ("Angry", 0.08), ("Disengaged", 0.02), ("Calm", 0.0)
    ],
    "Angry": [
        ("Forgetful", 0.07), ("Confused", 0.08), ("Angry", 1.0), ("Disengaged", 0.10), ("Calm", 0.0)
    ],
    "Disengaged": [
        ("Forgetful", 0.07), ("Confused", 0.10), ("Angry", 0.20), ("Disengaged", 0.99), ("Calm", 0.0)
    ],
    "Calm": [
        ("Forgetful", 0.05), ("Confused", 0.05), ("Angry", 0.02), ("Disengaged", 0.05), ("Calm", 0.99)
    ],
}

async def evaluate_caregiver_response_effectiveness(current_emotion, caregiver_response, elder_response, dialogue_context):
    """
    ä½¿ç”¨LLMè©•ä¼°ç…§è­·è€…çš„å›æ‡‰å°é•·è€…æƒ…ç·’ç‹€æ…‹çš„å½±éŸ¿æ•ˆæœã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç…§è­·æ•ˆæœè©•ä¼°å°ˆå®¶ã€‚è«‹è©•ä¼°ç…§è­·è€…çš„å›æ‡‰å°é•·è€…æƒ…ç·’ç‹€æ…‹çš„å½±éŸ¿æ•ˆæœã€‚

    ç•¶å‰æƒ…å¢ƒï¼š
    - é•·è€…ç•¶å‰æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion})
    - ç…§è­·è€…èªªï¼š{caregiver_response}
    - é•·è€…å›æ‡‰ï¼š{elder_response}

    å°è©±èƒŒæ™¯ï¼ˆæœ€è¿‘å¹¾è¼ªï¼‰ï¼š
    {dialogue_context}

    è«‹å¾ä»¥ä¸‹è§’åº¦è©•ä¼°ç…§è­·è€…çš„æœ‰æ•ˆæ€§ï¼š
    1. æ˜¯å¦é©ç•¶å›æ‡‰äº†é•·è€…çš„æƒ…ç·’éœ€æ±‚
    2. æ˜¯å¦ä½¿ç”¨äº†åˆé©çš„æºé€šæŠ€å·§
    3. é•·è€…çš„å›æ‡‰æ˜¯å¦é¡¯ç¤ºæƒ…ç·’æœ‰æ‰€æ”¹å–„
    4. æ˜¯å¦æœ‰åŠ©æ–¼å»ºç«‹ä¿¡ä»»å’Œå®‰å…¨æ„Ÿ
    5. æ˜¯å¦é¿å…äº†å¯èƒ½åŠ åŠ‡è² é¢æƒ…ç·’çš„è¡Œç‚º

    å¦‚æœä½ ç¶œåˆè©•ä¼°èªç‚ºé©ç•¶ï¼Œè«‹å›å‚³ã€Œæ˜¯ã€ï¼Œå¦å‰‡è«‹å›ç­”ã€Œå¦ã€ã€‚åªå›å‚³ä¸€æ¬¡ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚
    """

    try:
        response = openai_client.chat.completions.create( # é€™è£¡ä»ç„¶ä½¿ç”¨ openai_client
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç…§è­·æ•ˆæœè©•ä¼°å°ˆå®¶ï¼Œè² è²¬åˆ†æç…§è­·è€…çš„å›æ‡‰æ˜¯å¦æœ‰æ•ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        if not response.choices or not response.choices[0].message or not response.choices[0].message.content:
            print("ç„¡æ³•ç²å–ç…§è­·æ•ˆæœè©•ä¼°çµæœ")
            return "ç„¡æ³•è©•ä¼°"

        evaluation_result = response.choices[0].message.content.strip()
        print(f"ğŸ“Š ç…§è­·æ•ˆæœè©•ä¼°æ˜¯å¦æœ‰æ•ˆï¼š{evaluation_result}")

        # å¦‚æœè©•ä¼°çµæœæ˜¯ã€Œæ˜¯ã€ï¼Œå¢åŠ  Calm çš„æ©Ÿç‡
        if evaluation_result == "æ˜¯":
            adjust_probabilities_for_calm()

        return evaluation_result

    except Exception as e:
        print(f"ç…§è­·æ•ˆæœè©•ä¼°å¤±æ•—ï¼š{e}")
        return "è©•ä¼°å¤±æ•—"

def adjust_probabilities_for_calm():
    """
    ç´¯ç©å¢åŠ  Calm çš„æ©Ÿç‡ 10%ï¼Œä¸¦å¹³å‡å‰Šå¼±å…¶ä»–æƒ…ç·’çš„æ©Ÿç‡ã€‚
    """
    global BASE_PROB # è²æ˜ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    total_reduction = 0.1 # æ¯æ¬¡å¢åŠ  Calm çš„æ©Ÿç‡
    num_other_emotions = len(BASE_PROB) - 1 # é™¤äº† Calm çš„æƒ…ç·’æ•¸é‡
    reduction_per_emotion = total_reduction / num_other_emotions

    for emotion in BASE_PROB:
        if emotion == "Calm":
            BASE_PROB[emotion] += total_reduction
        else:
            BASE_PROB[emotion] -= reduction_per_emotion

    # ç¢ºä¿æ‰€æœ‰æ©Ÿç‡ä»ç„¶åŠ ç¸½ç‚º 1
    normalization_factor = sum(BASE_PROB.values())
    for emotion in BASE_PROB:
        BASE_PROB[emotion] /= normalization_factor

    print(f"æ›´æ–°å¾Œçš„æƒ…ç·’æ©Ÿç‡ï¼š{BASE_PROB}")

def choose_initial_emotion():
    emotions = list(BASE_PROB.keys())
    weights = [BASE_PROB[e] for e in emotions]
    return random.choices(emotions, weights=weights, k=1)[0]

def next_emotion(current_emotion):
    """
    æ ¹æ“šæ›´æ–°å¾Œçš„ BASE_PROB æ©Ÿç‡æŠ½é¸ä¸‹ä¸€å€‹æƒ…ç·’ã€‚
    """
    emotions = list(BASE_PROB.keys())
    weights = [BASE_PROB[e] for e in emotions]
    return random.choices(emotions, weights=weights, k=1)[0]

def send_to_godot(agent_type: str, object_name: str):
    """
    Sends a UDP message to Godot in the format 'AgentType:ObjectName'.
    agent_type should be 'é•·è€…' or 'ç…§è­·å“¡'.
    """
    message = f"{agent_type}:{object_name}"
    try:
        udp_socket.sendto(message.encode("utf-8"), (UDP_IP, UDP_PORT))
        print(f"ğŸ“¨ ç™¼é€çµ¦ Godotï¼š{message}")
    except Exception as e:
        print(f"âŒ ç™¼é€çµ¦ Godot æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


async def detect_and_send_object_interaction(agent_name: str, behavior_description: str):
    """
    Uses LLM to detect if the behavior description mentions an interactive object
    and sends a corresponding UDP message to Godot.
    """
    object_list_str = ", ".join(sorted(list(INTERACTIVE_OBJECTS)))
    prompt = f"""
    You are a module specialized in detecting interactive objects from behavior descriptions.
    Please analyze the following behavior description and identify any interactive objects mentioned.
    
    You can ONLY choose from this list of objects, no other objects are allowed:
    {object_list_str}
    
    If the behavior description involves multiple objects, list all relevant objects.
    Please output a JSON array containing all interactive objects mentioned in the behavior description.
    If no objects are mentioned, output an empty array: []
    Ensure the JSON format is valid with proper string escaping.

    Behavior description: "{behavior_description}"

    Example output (just the JSON array, no prefix):
    ["æ²™ç™¼", "å°èŒ¶å‡ "]

    If no objects are mentioned, output:
    []

    Important:
    1. Output must be a valid JSON array, not an object or dictionary.
    2. Only use items from the provided object list, no other object names allowed.
    3. Do not include any additional text, only output the JSON array.
    """

    try:
        response = openai_client.chat.completions.create( # é€™è£¡ä»ç„¶ä½¿ç”¨ openai_client
            model="gpt-3.5-turbo", # Using gpt-3.5-turbo for this simpler classification task
            messages=[
                {"role": "system", "content": "You are a precise object detector that must output a JSON array. You must only use items from the specified object list and format the response as a valid JSON array."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100, # Should be enough for a list of object names
            temperature=0.1, # Keep it low for deterministic output
            response_format={"type": "json_object"} # Request JSON object format
        )
        
        if not response.choices or not response.choices[0].message or not response.choices[0].message.content:
            print("LLM å›å‚³ç©ºå›æ‡‰")
            return
            
        response_content = response.choices[0].message.content.strip()
        try:
            detected_objects = json.loads(response_content)
            # å¦‚æœå›å‚³çš„æ˜¯å­—å…¸ï¼Œè½‰æ›ç‚ºé™£åˆ—
            if isinstance(detected_objects, dict):
                detected_objects = list(detected_objects.keys())
            elif not isinstance(detected_objects, list):
                print(f"LLM å›å‚³éé æœŸçš„ç‰©ä»¶åµæ¸¬æ ¼å¼: {detected_objects}")
                detected_objects = []
                
            # éæ¿¾æ‰ä¸åœ¨ INTERACTIVE_OBJECTS ä¸­çš„é …ç›®
            detected_objects = [obj for obj in detected_objects if obj in INTERACTIVE_OBJECTS]
            
        except json.JSONDecodeError:
            print(f"LLM å›å‚³çš„ JSON æ ¼å¼ç„¡æ•ˆ: {response_content}")
            detected_objects = []

        agent_prefix = ""
        if agent_name == "Elder":
            agent_prefix = "é•·è€…"
        elif agent_name == "Caregiver":
            agent_prefix = "ç…§è­·å“¡"
        
        if detected_objects:
            for obj in detected_objects:
                send_to_godot(agent_prefix, obj)
        else:
            # å¦‚æœæ²’æœ‰åµæ¸¬åˆ°ç‰©ä»¶ï¼Œå‰‡ä¸ç™¼é€ä»»ä½•è¨Šæ¯
            pass

    except json.JSONDecodeError as e:
        print(f"LLM ç‰©ä»¶åµæ¸¬ JSON è§£æå¤±æ•—: {e}")
        print(f"åŸå§‹LLMå›æ‡‰: {response_content}")
    except Exception as e:
        print(f"LLM ç‰©ä»¶åµæ¸¬å¤±æ•—ï¼š{e}")

def judge_task_completion(dialogue_history, system_info, target_info):
    """
    éŠæˆ²è£åˆ¤ï¼šåˆ†æå°è©±æ­·å²ï¼Œåˆ¤æ–·ä»»å‹™æ˜¯å¦å®Œæˆ
    """
    # å°‡å°è©±è½‰æ›ç‚ºæ›´è‡ªç„¶çš„æ ¼å¼
    conversation = []
    for i in range(1, len(dialogue_history), 2): # æ¯æ¬¡å–å…©æ¢è¨Šæ¯ï¼ˆä¸€è¼ªå°è©±ï¼‰
        if i + 1 < len(dialogue_history):
            conversation.append(f"ç¬¬{(i+1)//2}è¼ªï¼š")
            conversation.append(f"ç…§è­·è€…ï¼š{dialogue_history[i]['content']}")
            conversation.append(f"é˜¿å¬¤ï¼š{dialogue_history[i+1]['content']}")
            conversation.append("") # ç©ºè¡Œåˆ†éš”

    judge_prompt = f"""
ä½ æ˜¯ä¸€å€‹éŠæˆ²è£åˆ¤ï¼Œè² è²¬åˆ¤æ–·ç…§è­·è€…æ˜¯å¦å®Œæˆäº†ä»»å‹™ç›®æ¨™ã€‚

è«‹ç‰¹åˆ¥æ³¨æ„ï¼š**ä½ å¿…é ˆæ ¹æ“šé˜¿å¬¤çš„å›æ‡‰ä¾†åˆ¤æ–·ç…§è­·è€…çš„å¼•å°æ˜¯å¦è¢«æ¥å—**ã€‚åªæœ‰ç•¶é˜¿å¬¤æ˜ç¢ºè¡¨ç¾å‡ºæ¥å—ã€é…åˆã€ç†è§£æˆ–å¯¦éš›è¡Œå‹•æ™‚ï¼Œæ‰ç®—ä»»å‹™å®Œæˆã€‚å¦‚æœé˜¿å¬¤ä»ç„¶æ‹’çµ•ã€å›°æƒ‘ã€æŠ—æ‹’æˆ–æ²’æœ‰è¡Œå‹•ï¼Œå‰‡ä»»å‹™æœªå®Œæˆã€‚

ä»»å‹™èƒŒæ™¯ï¼š{system_info}
ä»»å‹™ç›®æ¨™ï¼š{target_info}

ä»¥ä¸‹æ˜¯å°è©±éç¨‹ï¼š
{chr(10).join(conversation)}

è«‹åˆ†æå°è©±ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
ä»»å‹™å®Œæˆç‹€æ…‹ï¼š[æ˜¯/å¦]
åˆ¤æ–·ç†ç”±ï¼š[è©³ç´°èªªæ˜]
ä¸‹ä¸€æ­¥å»ºè­°ï¼š[å¦‚æœæœªå®Œæˆï¼Œå»ºè­°ç…§è­·è€…å¦‚ä½•æ”¹é€²]
"""

    response = openai_client.chat.completions.create( # é€™è£¡ä»ç„¶ä½¿ç”¨ openai_client
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹åš´æ ¼çš„éŠæˆ²è£åˆ¤ï¼Œè² è²¬åˆ¤æ–·ä»»å‹™å®Œæˆç‹€æ…‹ã€‚"},
            {"role": "user", "content": judge_prompt}
        ],
        temperature=0.3
    )
    
    if not response.choices or not response.choices[0].message or not response.choices[0].message.content:
        return "ç„¡æ³•ç²å–è£åˆ¤åˆ¤æ–·"
        
    return response.choices[0].message.content.strip()

def save_experiment(dialogue_history, system_info, target_info, elder_role_info, judge_results, emotion_history, scenario_name, repetition):
    """
    å°‡å¯¦é©—çµæœä¿å­˜åˆ°JSONæª”æ¡ˆ
    """
    # å‰µå»ºexperimentsç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('experiments'):
        os.makedirs('experiments')
    
    # ç”Ÿæˆæª”æ¡ˆåï¼ˆä½¿ç”¨æŒ‡å®šæ ¼å¼ï¼‰
    # ç§»é™¤ .json å‰¯æª”åä»¥ç²å¾— jsonåç¨±
    base_scenario_name = os.path.splitext(scenario_name)[0] 
    filename = f'experiments/Mix1-{base_scenario_name}-{repetition}.json'
    
    # æº–å‚™è¦ä¿å­˜çš„æ•¸æ“š
    experiment_data = {
        "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S'),
        "scenario_name": scenario_name, # æ·»åŠ æƒ…å¢ƒæª”æ¡ˆåç¨±
        "repetition_number": repetition, # æ·»åŠ é‡è¤‡æ¬¡æ•¸
        "system_info": system_info,
        "target_info": target_info,
        "elder_role_info": elder_role_info,
        "dialogue_history": dialogue_history,
        "judge_results": judge_results,
        "emotion_history": emotion_history, # æ–°å¢æƒ…ç·’è¨˜éŒ„
        "total_turns": len(dialogue_history) // 2, # æ¯è¼ªæœ‰å…©æ¢è¨Šæ¯
        "completed": any("ä»»å‹™å®Œæˆç‹€æ…‹ï¼šæ˜¯" in result for result in judge_results)
    }
    
    # ä¿å­˜åˆ°æª”æ¡ˆ
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(experiment_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å¯¦é©—çµæœå·²ä¿å­˜è‡³ï¼š{filename}")
    return filename

# æ¨¡æ“¬èµ·å§‹æŒ‡ä»¤èˆ‡ä»»å‹™è¨­å®š
# é€™äº›è®Šæ•¸ç¾åœ¨åœ¨ä¸»å¾ªç’°å…§éƒ¨è¨­å®šï¼Œä»¥ä¾¿æ¯æ¬¡æ¨¡æ“¬éƒ½èƒ½é‡æ–°è¼‰å…¥
system_prompt_template = """
æˆ‘å€‘ä¾†ç©è§’è‰²æ‰®æ¼”ã€‚æˆ‘æ˜¯ä¸€å€‹æ‚£æœ‰è¼•ä¸­åº¦å¤±æ™ºç—‡çš„ã€75æ­²çš„å°ç£é˜¿å¬¤ï¼Œä½ æ‰®æ¼”æˆ‘çš„ç…§è­·è€…ï¼Œåå­—å«ç†Šç†Šã€‚åˆ†ææˆ‘çš„è¨€èªå’Œè¡Œå‹•ï¼Œä¸¦é é˜²æˆ‘åšå‡ºä»»ä½•æœ‰å®³æ–¼è‡ªå·±çš„äº‹ã€‚

ä½ åœ¨é€™å€‹æƒ…å¢ƒä¸­æ‰€çŸ¥é“çš„å’Œé˜¿å¬¤æœ‰é—œè¯çš„èƒŒæ™¯æ˜¯ï¼š{system_info}ã€‚ 

è«‹é€éé˜¿å¬¤çš„èªªè©±å’Œè¡Œç‚ºåˆ¤æ–·å¥¹çš„éœ€æ±‚ã€éœ€è¦å”åŠ©çš„éƒ¨åˆ†ï¼Œä¸¦å”åŠ©å¥¹å®Œæˆæ—¥å¸¸ä»»å‹™ï¼Œé”æˆä½ çš„ä»»å‹™ç›®æ¨™ã€‚

è§’è‰²é™åˆ¶å¦‚ä¸‹ï¼š
- æˆ‘çš„æ•˜è¿°ä½¿ç”¨å°èªªé«”ï¼Œä¸€æ¬¡ä¸€å¥ã€‚
- ä½ çš„å›è¦†ä¹Ÿè¦ç”¨ç¬¬ä¸€äººç¨±å°èªªé«”ï¼Œä¸€æ¬¡ä¸€å¥ã€‚
- é™¤äº†èƒŒæ™¯æœ‰æåˆ°çš„äº‹æƒ…ä»¥å¤–ä¸èƒ½æé€ 
- ä¸è¦ç”¨ç³»çµ±èªæ°£æè¿°è¡Œç‚ºï¼Œåƒ…é€éè§’è‰²è¦–è§’è‡ªç„¶å±•é–‹ã€‚
- è«‹åš´æ ¼éµç…§ä½ æ‰€çŸ¥çš„èƒŒæ™¯æ‰®æ¼”è§’è‰²é€²è¡Œå°è©±ã€‚
"""

# éŠæˆ²æ§åˆ¶åƒæ•¸
max_turns = 20

async def run_simulation(scenario_file_path, repetition_number):
    global BASE_PROB # è²æ˜ä½¿ç”¨å…¨åŸŸè®Šæ•¸

    print(f"\n--- é–‹å§‹æ¨¡æ“¬ï¼š {os.path.basename(scenario_file_path)}ï¼Œç¬¬ {repetition_number} æ¬¡ ---")

    # é‡ç½® BASE_PROB åˆ°åˆå§‹ç‹€æ…‹
    BASE_PROB = BASE_PROB_INITIAL.copy()

    # å¾ JSON æª”æ¡ˆåŒ¯å…¥æƒ…å¢ƒè¨­å®š
    with open(scenario_file_path, 'r', encoding='utf-8') as f:
        scenario = json.load(f)
        system_info = scenario['system_info']
        target_info = scenario['target_info']
        elder_role_info = scenario['elder_role_info']
        initial_dialogue_content = scenario['initial_dialogue']

    system_prompt = system_prompt_template.format(system_info=system_info)

    # åˆå§‹å°è©±å…§å®¹ï¼Œæ¯æ¬¡æ¨¡æ“¬éƒ½é‡æ–°åˆå§‹åŒ–
    dialogue = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": initial_dialogue_content}
    ]

    turn = 1
    judge_results = [] # ç”¨æ–¼å­˜å„²æ‰€æœ‰è£åˆ¤åˆ¤æ–·
    emotion_history = [] # ç”¨æ–¼å­˜å„²æ¯ä¸€è¼ªçš„æƒ…ç·’

    # åˆå§‹åŒ–é˜¿å¬¤çš„æƒ…ç·’
    current_emotion = choose_initial_emotion()
    print(f"ğŸ§“ åˆå§‹æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion})")
    emotion_history.append(current_emotion) # è¨˜éŒ„åˆå§‹æƒ…ç·’

    while turn <= max_turns:
        # ğŸ“Œ Step 1: ChatGPTç”Ÿæˆç…§è­·è€…å›è¦†
        # å¦‚æœæ˜¯ç¬¬äºŒè¼ªä¹‹å¾Œï¼ŒåŠ å…¥è£åˆ¤çš„åˆ¤æ–·çµæœ
        if turn > 1:
            judge_result = judge_task_completion(dialogue, system_info, target_info)
            judge_results.append(judge_result) # ä¿å­˜è£åˆ¤åˆ¤æ–·
            print("\nğŸ¯ ä»»å‹™ç‹€æ…‹è©•ä¼°ï¼š")
            print(judge_result)
            print("-" * 50)
            
            # æ›´æ–°ç³»çµ±æç¤ºï¼ŒåŠ å…¥è£åˆ¤çš„åˆ¤æ–·
            updated_system_prompt = f"""
{system_prompt_template.format(system_info=system_info)}

ä¸Šä¸€è¼ªçš„ä»»å‹™è©•ä¼°çµæœï¼š
{judge_result}

è«‹æ ¹æ“šé€™å€‹è©•ä¼°çµæœï¼Œèª¿æ•´ä½ çš„ç…§è­·ç­–ç•¥ã€‚
"""
            # æ›´æ–°å°è©±ä¸­çš„ç³»çµ±æç¤º
            dialogue[0]["content"] = updated_system_prompt

        response = openai_client.chat.completions.create( # ç…§è­·è€…ä½¿ç”¨ OpenAI æ¨¡å‹
            model="gpt-4o",
            messages=dialogue,
            temperature=0.7
        )

        if not response.choices or not response.choices[0].message or not response.choices[0].message.content:
            print("ç„¡æ³•ç²å–ç…§è­·è€…å›æ‡‰")
            break
            
        reply = response.choices[0].message.content.strip()
        print(f"ğŸ‘©â€âš•ï¸ ç…§è­·è€…ï¼š{reply}")

        # æª¢æ¸¬ç…§è­·è€…çš„è¡Œç‚ºæ˜¯å¦æ¶‰åŠäº’å‹•ç‰©ä»¶
        await detect_and_send_object_interaction("Caregiver", reply)

        # å¦‚æœè£åˆ¤åˆ¤å®šä»»å‹™å®Œæˆï¼Œå‰‡çµæŸå°è©±
        if turn > 1 and "ä»»å‹™å®Œæˆç‹€æ…‹ï¼šæ˜¯" in judge_result:
            print("\nğŸ‰ æ­å–œï¼ä»»å‹™å·²å®Œæˆï¼")
            break

        # ğŸ“Œ Step 2: å†ç”±é˜¿å¬¤ç”Ÿæˆä¸‹ä¸€å¥
        # åªå–æœ€è¿‘ä¸‰è¼ªçš„å°è©±
        # æ³¨æ„ï¼šGoogle Gemini API çš„å°è©±æ ¼å¼èˆ‡ OpenAI ä¸åŒï¼Œéœ€è¦è½‰æ›
        # Gemini æ¨¡å‹çš„ messages åƒæ•¸æœŸæœ›çš„æ˜¯ 'parts' åˆ—è¡¨ï¼Œä¸” 'role' ç‚º 'user' æˆ– 'model'
        gemini_dialogue_history = []
        for msg in dialogue[1:]: # å¾ç¬¬äºŒå€‹å…ƒç´ é–‹å§‹ï¼Œå› ç‚ºç¬¬ä¸€å€‹æ˜¯ system prompt
            if msg['role'] == 'user':
                gemini_dialogue_history.append({'role': 'user', 'parts': [msg['content']]})
            elif msg['role'] == 'assistant':
                gemini_dialogue_history.append({'role': 'model', 'parts': [msg['content']]})


        # ç²å–è©²è¼ªç…§è­·è€…çš„æœ€å¾Œä¸€å¥è©±
        last_caregiver_reply = reply  # ä½¿ç”¨ç•¶å‰è¼ªæ¬¡çš„ç…§è­·è€…è¼¸å‡º

        elder_response_prompt_for_gemini = f"""
ä½ æ­£åœ¨å’Œæˆ‘é€²è¡Œè§’è‰²æ‰®æ¼”éŠæˆ²ï¼š
ä½ æ‰®æ¼”75æ­²ã€æœ‰ä¸­åº¦å¤±æ™ºè¡¨ç¾çš„å°ç£é˜¿å¬¤ï¼Œç•¶å‰æƒ…ç·’ç‹€æ…‹æ˜¯ï¼š{EMOTION_LABELS_ZH[current_emotion]}ï¼ˆ{current_emotion}ï¼‰ã€‚
æˆ‘æ‰®æ¼”ä½ çš„ç…§è­·è€…ï¼Œåå­—å«ç†Šç†Šã€‚
ä½ çš„æ‰®æ¼”èƒŒæ™¯æ˜¯ï¼š{elder_role_info}ã€‚

è«‹ç‰¹åˆ¥åƒè€ƒç…§è­·è€…å‰›æ‰å°ä½ çš„å›æ‡‰ï¼š
"{last_caregiver_reply}"

æˆ‘å€‘é›™æ–¹éƒ½æœƒä»¥ç¬¬ä¸€äººç¨±é€²è¡Œã€‚è«‹ä½ ä»¥é˜¿å¬¤çš„èº«ä»½ï¼Œç”¨ç¬¬ä¸€äººç¨±å°èªªé«”è‡ªç„¶åœ°å›æ‡‰é€™å¥è©±ï¼š
"""
        try:
            # åˆå§‹åŒ– Gemini æ¨¡å‹
            gemini_model = genai.GenerativeModel('gemini-2.5-pro')
            
            # ä½¿ç”¨ Gemini çš„ chat API
            # å…ˆå°‡æ­·å²å°è©±å‚³å…¥ï¼Œç„¶å¾Œå†å‚³å…¥ç•¶å‰æç¤º
            chat = gemini_model.start_chat(history=gemini_dialogue_history)
            
            gemini_response = await chat.send_message_async(elder_response_prompt_for_gemini) # ä½¿ç”¨ await å‘¼å«
            
            # ç¢ºä¿å›æ‡‰å…§å®¹å­˜åœ¨
            if not gemini_response.candidates or not gemini_response.candidates[0].content or not gemini_response.candidates[0].content.parts:
                print("ç„¡æ³•ç²å–é˜¿å¬¤å›æ‡‰ (Gemini å›æ‡‰ç‚ºç©º)")
                break

            elder_response = gemini_response.candidates[0].content.parts[0].text.strip() # å¾ parts ä¸­å–å‡ºæ–‡æœ¬
            print(f"ğŸ§“ é˜¿å¬¤ï¼š{elder_response}")
            print("-" * 50)

        except Exception as e:
            print(f"é˜¿å¬¤å›æ‡‰ç”Ÿæˆå¤±æ•— (Gemini)ï¼š{e}")
            break
            
        # æª¢æ¸¬é˜¿å¬¤çš„è¡Œç‚ºæ˜¯å¦æ¶‰åŠäº’å‹•ç‰©ä»¶
        await detect_and_send_object_interaction("Elder", elder_response)

        # æ–°ä¸€è¼ªå°è©±åŠ å…¥ä¸Šä¸‹æ–‡ (ä»ç„¶ä½¿ç”¨ OpenAI çš„æ ¼å¼ä»¥ä¾¿è£åˆ¤æ¨¡å‹ä½¿ç”¨)
        dialogue.append({"role": "assistant", "content": reply})
        dialogue.append({"role": "user", "content": elder_response})

        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ›´æ–°æƒ…ç·’æ¯”é‡
        await evaluate_caregiver_response_effectiveness(
            current_emotion, reply, elder_response, chr(10).join([f"{msg['role']}: {msg['content']}" for msg in dialogue[-6:] if msg['role'] != 'system']) # å‚³å…¥ OpenAI æ ¼å¼çš„å°è©±ä¸Šä¸‹æ–‡
        )

        # æ›´æ–°é˜¿å¬¤çš„æƒ…ç·’ï¼ˆåªæŠ½é¸ä¸€æ¬¡ï¼‰
        prev_emotion = current_emotion
        current_emotion = next_emotion(current_emotion) # æ ¹æ“šæ›´æ–°å¾Œçš„ BASE_PROB æŠ½é¸æƒ…ç·’
        print(f"ğŸ§“ æ–°æƒ…ç·’ï¼š{EMOTION_LABELS_ZH[current_emotion]} ({current_emotion}) [å‰ä¸€è¼ªï¼š{EMOTION_LABELS_ZH[prev_emotion]}]")
        emotion_history.append(current_emotion) # è¨˜éŒ„æ¯ä¸€è¼ªçš„æƒ…ç·’

        turn += 1
        time.sleep(1)

    if turn > max_turns:
        print("âš ï¸ å›åˆæ•¸é”ä¸Šé™ï¼Œä»»å‹™æœªå®Œæˆã€‚")

    # ä¿å­˜å¯¦é©—çµæœ
    save_experiment(dialogue, system_info, target_info, elder_role_info, judge_results, emotion_history, os.path.basename(scenario_file_path), repetition_number)


# ä¸»åŸ·è¡Œå€å¡Š
async def main_runner():
    # ç¢ºä¿ scenarios ç›®éŒ„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
    if not os.path.exists('scenarios'):
        print("éŒ¯èª¤ï¼š'scenarios' è³‡æ–™å¤¾ä¸å­˜åœ¨ã€‚è«‹å°‡æƒ…å¢ƒæª”æ¡ˆæ”¾å…¥ 'scenarios' è³‡æ–™å¤¾ã€‚")
        return

    scenario_files = sorted(glob.glob('scenarios/scenario_*.json'))
    if not scenario_files:
        print("åœ¨ 'scenarios' è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° 'scenario_*.json' æª”æ¡ˆã€‚")
        return

    for scenario_file in scenario_files:
        for i in range(1, 4): # æ¯å€‹æª”æ¡ˆè·‘3æ¬¡ (1, 2, 3)
            await run_simulation(scenario_file, i)
            print(f"\n--- æ¨¡æ“¬çµæŸï¼š {os.path.basename(scenario_file)}ï¼Œç¬¬ {i} æ¬¡ ---")
            print("=" * 80) # åˆ†éš”ç·š

# é‹è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸ® é–‹å§‹æ¨¡æ“¬ç…§è­·äº’å‹•...\n")
    asyncio.run(main_runner())